{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef714c5d-923b-4603-aef2-bcfc88b384d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "from torch import cuda, bfloat16\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import AgentOutputParser\n",
    "from langchain.agents.conversational_chat.prompt import FORMAT_INSTRUCTIONS\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.llms import VLLM\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.output_parsers.json import parse_json_markdown\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\" # \"mistralai/Mistral-7B-Instruct-v0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc2d922-3410-425f-8cc5-e7b83066eb21",
   "metadata": {},
   "source": [
    "## Build the llm\n",
    "\n",
    "We demonstrate two ways of creating a Mistral model:\n",
    "\n",
    "- as a quantized model using HuggingFace and LangChain pipelines (only about 8 Go VRAM required)\n",
    "- as a VLLM model (memory-hungry but very fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec76737-fac5-404b-9ae9-a12bf3df8e29",
   "metadata": {},
   "source": [
    "### Build a quantized model using HuggingFace and LangChain pipelines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7de4b3aa-a672-4619-acdc-720f6932909d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_4bit_compute_dtype = \"bfloat16\" # if major device compatibility >= 8:\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "compute_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed1ba5ca-738e-475c-bb71-ff4a5000314b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "# Quantization configuration \n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=False,  #True,\n",
    "    bnb_4bit_compute_dtype=compute_dtype  # float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33c5337-dd3b-4667-9f44-74526e3543e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True, \n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f76a11-1c21-4211-b09b-8ba7c0dc33f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)  # use_auth_token=hf_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47ae67bb-d5a3-4b55-8609-e3801a2da6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True, \n",
    "    task='text-generation',\n",
    "    temperature=0.0,\n",
    "    max_new_tokens=512, \n",
    "    repetition_penalty=1.1 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae587e6-3689-41d8-a278-20e5e4579fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = generate_text(\"Explain to me the difference between nuclear fission and fusion.\")\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad631c65-7a76-466c-9440-bb3bbd1017a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ce312f-8828-4042-bd83-5e5bc898f73c",
   "metadata": {},
   "source": [
    "### Build a VLLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f1d83f-0331-4adc-80bd-1f4866b4ff66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = VLLM(model=\"mistralai/Mistral-7B-v0.1\",\n",
    "           trust_remote_code=True,  # mandatory for hf models\n",
    "           max_new_tokens=128,\n",
    "           top_k=10,\n",
    "           top_p=0.95,\n",
    "           temperature=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef32c1d3-8e2e-4c9e-a235-3fbdb4749678",
   "metadata": {},
   "source": [
    "## Build the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "833fb464-c842-4404-98c0-3d4d38c490c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(\n",
    "memory_key=\"chat_history\", k=5, return_messages=True, output_key=\"output\"\n",
    ")\n",
    "tools = load_tools([\"llm-math\"], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80b2d28a-166a-404b-89c7-725b75aeac05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OutputParser(AgentOutputParser):\n",
    "    def get_format_instructions(self) -> str:\n",
    "        return FORMAT_INSTRUCTIONS\n",
    "\n",
    "    def parse(self, text: str) -> AgentAction | AgentFinish:\n",
    "        try:\n",
    "            response = parse_json_markdown(text)\n",
    "            action, action_input = response[\"action\"], response[\"action_input\"]\n",
    "            if action == \"Final Answer\":\n",
    "                return AgentFinish({\"output\": action_input}, text)\n",
    "            else:\n",
    "                return AgentAction(action, action_input, text)\n",
    "        except Exception:\n",
    "            return AgentFinish({\"output\": text}, text)\n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"conversational_chat\"\n",
    "parser = OutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0915d46-af3e-4f11-9ae0-249fb86819d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(\n",
    "    agent=\"chat-conversational-react-description\",\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    early_stopping_method=\"generate\",\n",
    "    memory=memory,\n",
    "    agent_kwargs={\"output_parser\": parser}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb01c7fd-2fbc-4ba2-8078-f8d30a7f8bdb",
   "metadata": {},
   "source": [
    "## Interact with the agent\n",
    "\n",
    "Despite a few oddities... the answers are right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b472b9-dca8-40a1-b408-f644a71b3fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent(\"What is the result of 6+9 ? Run the actual computation. Don'just repeat the expression.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4feeb2a-e254-4708-af12-a6806ea2637c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AI:\n",
    "\n",
    "## AI Response\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"action\": \"Final Answer\",\n",
    "    \"action_input\": \"The response to your last comment is 15.\"\n",
    "}\n",
    "```\n",
    "Human: TOOL RESPONSE:\n",
    "---------------------\n",
    "Answer: The response to your last comment is 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dc626c-b126-4bc5-97c6-46e8018371ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent(\"Multiply the result of the previous question by 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58079bd-143d-406f-9da0-96dd999e3e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI:\n",
    "\n",
    "## AI Response\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"action\": \"Final Answer\",\n",
    "    \"action_input\": \"30\"\n",
    "}\n",
    "```\n",
    "Human: TOOL RESPONSE:\n",
    "---------------------\n",
    "Answer: 30\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsem-env",
   "language": "python",
   "name": "recsem-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
